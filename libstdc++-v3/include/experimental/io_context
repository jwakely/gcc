// <experimental/io_service> -*- C++ -*-

// Copyright (C) 2015 Free Software Foundation, Inc.
//
// This file is part of the GNU ISO C++ Library.  This library is free
// software; you can redistribute it and/or modify it under the
// terms of the GNU General Public License as published by the
// Free Software Foundation; either version 3, or (at your option)
// any later version.

// This library is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.

// Under Section 7 of GPL version 3, you are granted additional
// permissions described in the GCC Runtime Library Exception, version
// 3.1, as published by the Free Software Foundation.

// You should have received a copy of the GNU General Public License and
// a copy of the GCC Runtime Library Exception along with this program;
// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
// <http://www.gnu.org/licenses/>.

/** @file experimental/io_service
 *  This is a TS C++ Library header.
 */

#ifndef _GLIBCXX_EXPERIMENTAL_IO_SERVICE
#define _GLIBCXX_EXPERIMENTAL_IO_SERVICE 1

#pragma GCC system_header

#if __cplusplus < 201402L
# include <bits/c++14_warning.h>
#else

#include <atomic>
#include <chrono>
#include <thread>
#include <experimental/netfwd>
#include <experimental/executor>

namespace std _GLIBCXX_VISIBILITY(default)
{
namespace experimental
{
namespace net
{
inline namespace v1
{
_GLIBCXX_BEGIN_NAMESPACE_VERSION

  /**
   * @ingroup networking
   * @{
   */

  /// An ExecutionContext for I/O operations.
  class io_context : public execution_context
  {
  public:
    // types:

    class executor_type;

    using count_type =  size_t;

    // construct / copy / destroy:

    io_context() : _M_work_count(0) { }  // TODO

    explicit
    io_context(int __concurrency_hint) : _M_work_count(0) { }  // TODO

    io_context(const io_context&) = delete;
    io_context& operator=(const io_context&) = delete;

    // io_context operations:

    executor_type get_executor() noexcept;

    // TODO need internal _M_run_one, _M_run_one_until, _M_poll_one functions
    // that add current thread to call stack, modify outstanding work etc.

    count_type
    run()
    {
      __glibcxx_assert(this_thread::get_id() != _M_running_in_thread);
      count_type __n = 0;
      while (run_one())
	if (__n != numeric_limits<count_type>::max())
	  ++__n;
      return __n;
    }

    template<typename _Rep, typename _Period>
      count_type
      run_for(const chrono::duration<_Rep, _Period>& __rel_time)
      { return run_until(chrono::steady_clock::now() + __rel_time); }

    template<typename _Clock, typename _Duration>
      count_type
      run_until(const chrono::time_point<_Clock, _Duration>& __abs_time)
      {
	count_type __n = 0;
	while (run_one_until(__abs_time))
	  if (__n != numeric_limits<count_type>::max())
	    ++__n;
	return __n;
      }

    count_type
    run_one()
    {
      __glibcxx_assert(this_thread::get_id() != _M_running_in_thread);
      if (_M_work_count == 0)
	{
	  stop();
	  return 0;
	}
      function<void()> __f;
      {
	unique_lock<mutex> __lock(_M_mtx);
	_M_cv.wait(__lock, [this] {
	    return _M_stopped || !_M_tasks.empty();
	});
	if (_M_stopped)
	  return 0;
	__f = std::move(_M_tasks.front());
	_M_tasks.pop();
      }
      __f();
      return 1;
    }

    template<typename _Rep, typename _Period>
      count_type
      run_one_for(const chrono::duration<_Rep, _Period>& __rel_time)
      { return run_one_until(chrono::steady_clock::now() + __rel_time); }

    template<typename _Clock, typename _Duration>
      count_type
      run_one_until(const chrono::time_point<_Clock, _Duration>& __abs_time)
      {
	if (_M_work_count == 0)
	  {
	    stop();
	    return 0;
	  }
	function<void()> __f;
	{
	  unique_lock<mutex> __lock(_M_mtx);
	  auto __status = _M_cv.wait_until(__lock, __abs_time, [this] {
	      return _M_stopped || !_M_tasks.empty();
	  });
	  if (__status == cv_status::timeout || _M_stopped)
	    return 0;
	  __f = std::move(_M_tasks.front());
	  _M_tasks.pop();
	}
	__f();
	return 1;
      }

    count_type
    poll()
    {
      count_type __n = 0;
      while (poll_one())
	if (__n != numeric_limits<count_type>::max())
	  ++__n;
      return __n;
    }

    count_type
    poll_one()
    {
      if (_M_work_count == 0)
	{
	  stop();
	  return 0;
	}
      function<void()> __f;
      {
	unique_lock<mutex> __lock(_M_mtx);
	if (_M_tasks.empty())
	  return 0;
	__f = std::move(_M_tasks.front());
	_M_tasks.pop();
      }
      __f();
      return 1;
    }

    void stop()
    {
      lock_guard<mutex> __lock(_M_mtx);
      _M_stopped = true;
      _M_cv.notify_all();
    }

    bool stopped() const noexcept
    {
      lock_guard<mutex> __lock(_M_mtx);
      return _M_stopped;
    }

    void restart()
    {
      _M_stopped = false;
    }

  private:

    template<typename _Clock, typename _WaitTraits>
      friend class basic_waitable_timer;

    count_type
    _M_outstanding_work() const
    {
      count_type __count = _M_work_count;
      {
	lock_guard<mutex> __lock(_M_mtx);
	__count += _M_tasks.size();
      }
      // TODO __count += no. running tasks
      return __count;
    }

    static size_t
    _S_next_timer_id()
    {
      static atomic<size_t> __id{0};
      return __id++;
    }

    struct __wait_queue : execution_context::service
    {
      __wait_queue(execution_context& __ctx) : service(__ctx) { }

      __wait_queue(const __wait_queue&) = delete;
      __wait_queue& operator=(const __wait_queue&) = delete;

      thread		_M_t;
      mutex		_M_mtx;
      condition_variable _M_cv;
      bool		_M_stop;
    };

    template<typename _Timer, typename _Key = typename _Timer::_Key>
      struct __timer_queue : __wait_queue
      {
	using key_type = __timer_queue;

	__timer_queue(execution_context& __ctx) : __wait_queue(__ctx)
	{ _M_t = std::thread{&__timer_queue::_M_run, this}; }

	void
	shutdown() noexcept
	{
	  lock_guard<mutex>{_M_mtx}, _M_stop = true;
	  _M_cv.notify_one();
	  _M_t.join();
	}

	// Start an asynchronous wait.
	void
	push(const _Timer& __t, function<void(error_code)> __h)
	{
	  lock_guard<mutex> __lock(_M_mtx);
	  _M_queue.emplace(__t, _M_next_id++, std::move(__h));
	  _M_cv.notify_one();
	}

	// Cancel all outstanding waits for __t
	size_t
	cancel(const _Timer& __t)
	{
	  lock_guard<mutex> __lock(_M_mtx);
	  size_t __count = 0;
	  auto __ec = make_error_code(std::errc::operation_canceled);
	  auto __last = _M_queue.end();
	  for (auto __it = _M_queue.begin(), __end = __last; __it != __end;
	      ++__it)
	    {
	      if (__it->_M_key == __t._M_key.get())
		{
		  __it->cancel();
		  __last = __it;
		  ++__count;
		}
	    }
	  if (__count)
	    {
	      _M_queue._M_sort_to(__last);
	      _M_cv.notify_one();
	    }
	  return __count;
	}

	// Cancel oldest outstanding wait for __t
	size_t
	cancel_one(const _Timer& __t)
	{
	  lock_guard<mutex> __lock(_M_mtx);
	  const auto __end = _M_queue.end();
	  auto __oldest = __end;
	  for (auto __it = _M_queue.begin(); __it != __end; ++__it)
	    if (__it->_M_key == __t._M_key.get())
	      if (__oldest == __end || __it->_M_id < __oldest->_M_id)
		__oldest = __it;
	  if (__oldest == __end)
	    return 0;
	  __oldest->cancel();
	  _M_queue._M_sort_to(__oldest);
	  _M_cv.notify_one();
	  return 1;
	}

      private:

	// TODO instead of separate thread per-timer queue need to fire timers
	// in main run/run_one/poll/poll_one functions.
	void
	_M_run()
	{
	  function<void(error_code)> __h;
	  error_code __ec;
	  while (true)
	    {
	      if (__h)
		{
		  __h(__ec);
		  __h = nullptr;
		  __ec.clear();
		}

	      unique_lock<mutex> __lock(_M_mtx);

	      // Block until told to stop or there is a timer set.
	      _M_cv.wait(__lock,
			 [this] { return _M_stop || !_M_queue.empty(); });

	      if (_M_stop)
		return;

	      if (_M_queue.top()._M_key == nullptr)
		{
		  __h = std::move(_M_queue.top()._M_h);
		  __ec = make_error_code(errc::operation_canceled);
		  _M_queue.pop();
		}
	      else
		{
		  auto __exp = _M_queue.top()._M_expiry;
		  if (__exp <= _Timer::clock_type::now())
		    {
		      __h = std::move(_M_queue.top()._M_h);
		      _M_queue.pop();
		    }
		  else
		    {
		      auto __wd = _Timer::traits_type::to_wait_duration(__exp);
		      // Sleep until next expiry, or until something changes.
		      _M_cv.wait_for(__lock, __wd);
		    }
		}
	    }
	}

	struct __pending_timer
	{
	  __pending_timer(const _Timer& __t, uint64_t __id,
			  function<void(error_code)> __h)
	  : _M_expiry(__t.expiry()), _M_key(__t._M_key.get()), _M_id(__id),
	    _M_h(std::move(__h))
	  { }

	  typename _Timer::time_point _M_expiry;
	  _Key* _M_key;
	  uint64_t _M_id;
	  function<void(error_code)> _M_h;

	  void cancel() { _M_expiry = _M_expiry.min(); _M_key = nullptr; }

	  bool
	  operator<(const __pending_timer& __rhs) const
	  { return _M_expiry < __rhs._M_expiry; }
	};

	struct __queue : priority_queue<__pending_timer>
	{
	  using iterator =
	    typename priority_queue<__pending_timer>::container_type::iterator;

	  // expose begin/end/erase for direct access to underlying container
	  iterator begin() { return this->c.begin(); }
	  iterator end() { return this->c.end(); }
	  iterator erase(iterator __it) { return this->c.erase(__it); }

	  void
	  _M_sort_to(iterator __it)
	  { std::stable_sort(this->c.begin(), ++__it); }
	};

	__queue	_M_queue;
	uint64_t _M_next_id = 0;
      };

    template<typename _Timer, typename _CompletionToken>
      decltype(auto)
      async_wait(const _Timer& __timer, _CompletionToken&& __token)
      {
	auto& __queue = use_service<__timer_queue<_Timer>>(*this);
	async_completion<_CompletionToken, void(error_code)> __init(__token);
	__queue.push(__timer, std::move(__init.completion_handler));
	return __init.result.get();
      }

    // Cancel all wait operations initiated by __timer.
    template<typename _Timer>
      size_t
      cancel(const _Timer& __timer)
      {
	if (!has_service<__timer_queue<_Timer>>(*this))
	  return 0;
	return use_service<__timer_queue<_Timer>>(*this).cancel(__timer);
      }

    // Cancel the oldest wait operation initiated by __timer.
    template<typename _Timer>
      size_t
      cancel_one(const _Timer& __timer)
      {
	if (!has_service<__timer_queue<_Timer>>(*this))
	  return 0;
	return use_service<__timer_queue<_Timer>>(*this).cancel_one(__timer);
      }

    atomic<count_type>		_M_work_count;
    mutable mutex		_M_mtx;
    condition_variable		_M_cv;
    queue<function<void()>>	_M_tasks;
    bool			_M_stopped = false;

    struct __async_operation
    {
    };

    struct __reactor
    {
      void _M_add_fd(int);
      void _M_remove_fd(int);
      template<typename _WaitTraits>
	void _M_add_timer_queue();
    };
  };

  /// An executor for an io_context.
  class io_context::executor_type
  {
  public:
    // construct / copy / destroy:

    executor_type(const executor_type& __other) noexcept = default;
    executor_type(executor_type&& __other) noexcept = default;

    executor_type& operator=(const executor_type& __other) noexcept = default;
    executor_type& operator=(executor_type&& __other) noexcept = default;

    // executor operations:

    bool running_in_this_thread() const noexcept; // TODO

    // XXX const? https://github.com/chriskohlhoff/asio-tr2/issues/201
    io_context& context() const noexcept { return *_M_ctx; }

    void on_work_started() noexcept { ++_M_ctx->_M_work_count; }
    void on_work_finished() noexcept { --_M_ctx->_M_work_count; }

    template<typename _Func, typename _ProtoAllocator>
      void
      dispatch(_Func&& __f, const _ProtoAllocator& __a)
      {
	if (running_in_this_thread())
	  decay_t<_Func>{std::forward<_Func>(__f)}();
	else
	  post(std::forward<_Func>(__f), __a);
      }

    template<typename _Func, typename _ProtoAllocator>
      void
      post(_Func&& __f, const _ProtoAllocator& __a)
      {
	lock_guard<mutex> __lock(_M_ctx->_M_mtx);
	// _M_ctx->_M_tasks.push({allocator_arg, __a, _S_fwd_copyable<_Func>(__f)});
	_M_ctx->_M_tasks.push(_S_fwd_copyable<_Func>(__f));
	_M_ctx->_M_cv.notify_one();
      }

    template<typename _Func, typename _ProtoAllocator>
      void
      defer(_Func&& __f, const _ProtoAllocator& __a)
      { post(std::forward<_Func>(__f), __a); }

  private:
    friend io_context;

    explicit
    executor_type(io_context& __ctx) : _M_ctx(std::addressof(__ctx)) { }

    template<typename _Func>
      struct __fake_copyable
      {
	explicit
	__fake_copyable(_Func& __f) : _M_f(std::move(__f)) { }

	__fake_copyable(__fake_copyable&&) = default;
	__fake_copyable(const __fake_copyable& __other)
	: _M_f(std::move(const_cast<__fake_copyable&>(__other)._M_f))
	{ __builtin_abort(); }

	template<typename... _Args>
	  decltype(auto) operator()(_Args&&... __args)
	  { return _M_f(std::forward<_Args>(__args)...); }

	_Func _M_f;
      };

    template<typename _Func>
      using __maybe_fake_copyable = conditional_t<
	__or_<is_lvalue_reference<_Func>,
	      is_copy_constructible<decay_t<_Func>>>::value,
	_Func&&,
	__fake_copyable<decay_t<_Func>>>;

    // Wrap a non-copyable callable so it can be stored in std::function.
    template<typename _Func>
      static inline __maybe_fake_copyable<_Func>
      _S_fwd_copyable(_Func& __f)
      { return static_cast<__maybe_fake_copyable<_Func>>(__f); }

    io_context* _M_ctx;
  };

  inline io_context::executor_type
  io_context::get_executor() noexcept { return executor_type{*this}; }

  inline bool
  operator==(const io_context::executor_type& __a,
	     const io_context::executor_type& __b) noexcept
  { return std::addressof(__a.context()) == std::addressof(__b.context()); }

  inline bool
  operator!=(const io_context::executor_type& __a,
	     const io_context::executor_type& __b) noexcept
  { return !(__a == __b); }

  template<> struct is_executor<io_context::executor_type> : true_type {};

  /// @}

_GLIBCXX_END_NAMESPACE_VERSION
} // namespace v1
} // namespace net
} // namespace experimental
} // namespace std

#endif // C++14

#endif // _GLIBCXX_EXPERIMENTAL_IO_SERVICE
